{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization in PyTorch Lightning with Optuna\n",
    "\n",
    "In notebook 4 of this exercise, you've learned how to develop and train models with PyTorch Lightning.\n",
    "\n",
    "In this optional notebook, we'll show you how you can perform hyperparameter optimization in PyTorch Lightning using the  framework *Optuna*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "#!pip install pytorch-lightning==0.7.6 > /dev/null\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from exercise_code.MyPytorchModel import MyPytorchModel\n",
    "from exercise_code.Util import save_model, load_model, test_and_save\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://raw.githubusercontent.com/optuna/optuna/master/docs/image/optuna-logo.png></img>\n",
    "\n",
    "Optuna is an automatic hyperparameter optimization framework, which works with several deep learning libraries, including PyTorch Lightning. Have a look at https://github.com/optuna/optuna!\n",
    "\n",
    "Two important concepts of Optuna are the terms **Study** and **Trial**:\n",
    "* **Study**: optimization based on an objective function (e.g.: maximize validation accuracy)\n",
    "* **Trial**: a single execution of the objective function (i.e., train a model for a specific hyperparameter configuration)\n",
    "\n",
    "The goal of a study is to find out the optimal set of hyperparameter values through multiple trials (e.g., n_trials=100). Optuna is a framework designed for the automation and the acceleration of the optimization studies.\n",
    "\n",
    "On a high level, hyper parameter tuning with Optuna works very similar to the search algorithms we implemented in exercise 6. However, Optuna has a set of advantages:\n",
    "\n",
    "* **Parallelization** of hyperparameter searches over multiple threads or processes without modifying code\n",
    "* more **efficient search algorithms** for large search spaces and **pruning of unpromising trials** for faster results\n",
    "* many additional features for automated search of optimal hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install & Import Optuna\n",
    "\n",
    "If you haven't done yet, install Optuna via pip by uncommenting the line in the cell below: `!pip install optuna`.\n",
    "\n",
    "If you want to install Optuna with conda or you have problems with the installation via pip, you can also use `$ conda install -c conda-forge optuna`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-1.5.0.tar.gz (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 111 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting alembic\n",
      "  Downloading alembic-1.4.2.tar.gz (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 314 kB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cliff\n",
      "  Downloading cliff-3.2.0-py3-none-any.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 170 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cmaes>=0.5.0\n",
      "  Downloading cmaes-0.5.0-py3-none-any.whl (13 kB)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-4.1.0-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.7/site-packages (from optuna) (0.14.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from optuna) (1.18.1)\n",
      "Requirement already satisfied: scipy!=1.4.0 in /opt/anaconda3/lib/python3.7/site-packages (from optuna) (1.4.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /opt/anaconda3/lib/python3.7/site-packages (from optuna) (1.3.13)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.7/site-packages (from optuna) (4.42.1)\n",
      "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.7/site-packages (from alembic->optuna) (2.8.1)\n",
      "Collecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.1.3-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 575 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=3.12 in /opt/anaconda3/lib/python3.7/site-packages (from cliff->optuna) (5.3)\n",
      "Collecting stevedore>=1.20.0\n",
      "  Downloading stevedore-2.0.0-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 571 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cmd2!=0.8.3,<0.9.0,>=0.8.0\n",
      "  Downloading cmd2-0.8.9-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 530 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /opt/anaconda3/lib/python3.7/site-packages (from cliff->optuna) (1.14.0)\n",
      "Collecting PrettyTable<0.8,>=0.7.2\n",
      "  Downloading prettytable-0.7.2.tar.bz2 (21 kB)\n",
      "Requirement already satisfied: pyparsing>=2.1.0 in /opt/anaconda3/lib/python3.7/site-packages (from cliff->optuna) (2.4.6)\n",
      "Collecting pbr!=2.1.0,>=2.0.0\n",
      "  Downloading pbr-5.4.5-py2.py3-none-any.whl (110 kB)\n",
      "\u001b[K     |████████████████████████████████| 110 kB 206 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/anaconda3/lib/python3.7/site-packages (from Mako->alembic->optuna) (1.1.1)\n",
      "Requirement already satisfied: wcwidth; sys_platform != \"win32\" in /opt/anaconda3/lib/python3.7/site-packages (from cmd2!=0.8.3,<0.9.0,>=0.8.0->cliff->optuna) (0.1.8)\n",
      "Collecting pyperclip\n",
      "  Downloading pyperclip-1.8.0.tar.gz (16 kB)\n",
      "Building wheels for collected packages: optuna, alembic, PrettyTable, pyperclip\n",
      "  Building wheel for optuna (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for optuna: filename=optuna-1.5.0-py3-none-any.whl size=276142 sha256=ee88eeec58d0e58cfc6056c850e29ac536b6a7a559ea23e7034de36a0eba74e4\n",
      "  Stored in directory: /Users/dddong/Library/Caches/pip/wheels/0a/f9/1b/1c068c5fb648ad0a4600c104495c61ae780a2f3bcf52bcd676\n",
      "  Building wheel for alembic (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for alembic: filename=alembic-1.4.2-py2.py3-none-any.whl size=159543 sha256=937eedf47be5986aac4a288a06257ee031db522af675718860bb4f19ea7fcbd1\n",
      "  Stored in directory: /Users/dddong/Library/Caches/pip/wheels/4e/b5/00/f93fe1c90b3d501774e91e2e99987f49d16019e40e4bd3afc3\n",
      "  Building wheel for PrettyTable (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PrettyTable: filename=prettytable-0.7.2-py3-none-any.whl size=13698 sha256=3a563f7a9fa95e79e92b0a549daacd1fca1208cd3816ee9f31405b544dedf126\n",
      "  Stored in directory: /Users/dddong/Library/Caches/pip/wheels/8c/76/0b/eb9eb3da7e2335e3577e3f96a0ae9f74f206e26457bd1a2bc8\n",
      "  Building wheel for pyperclip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyperclip: filename=pyperclip-1.8.0-py3-none-any.whl size=8691 sha256=6f5d4913e6c37b5a5b5d72f7741754b0a6a35df126f1f70b74507636017cbe9f\n",
      "  Stored in directory: /Users/dddong/Library/Caches/pip/wheels/e5/5e/f7/441179ddf6ac56f36cb1d84d94f35beedd5da15986ce3d321d\n",
      "Successfully built optuna alembic PrettyTable pyperclip\n",
      "Installing collected packages: python-editor, Mako, alembic, pbr, stevedore, pyperclip, cmd2, PrettyTable, cliff, cmaes, colorlog, optuna\n",
      "Successfully installed Mako-1.1.3 PrettyTable-0.7.2 alembic-1.4.2 cliff-3.2.0 cmaes-0.5.0 cmd2-0.8.9 colorlog-4.1.0 optuna-1.5.0 pbr-5.4.5 pyperclip-1.8.0 python-editor-1.0.4 stevedore-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger\n",
    "\n",
    "The default logger in PyTorch Lightning automatically writes to event files to be consumed by TensorBoard. \n",
    "\n",
    "Here we just set up a simple callback, that keeps the metrics from each validation step in memory, which we can then use to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Callback\n",
    "\n",
    "class MetricsCallback(Callback):\n",
    "    \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = []\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        self.metrics.append(trainer.callback_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "As mentioned above, the goal of our hyperparameter tuning is, to optimize an objective function by finding the best set of hyperparameters. \n",
    "(\n",
    "Optuna is a black-box optimizer, which means we need to provide this objective function (`objective(trial)`), which gets passed a `trial` object and returns a numerical value to evaluate the performance of the current hyperparameters and where to sample in the upcoming trial.\n",
    "\n",
    "In our case, the metric we want to optimize is the validation accuracy of our models. To get the validation accuracy for a given hyperparameter configuration, we do the same thing as in the previous notebook:\n",
    "\n",
    "* define a PL-trainer\n",
    "* define hyper parameters - by sampling them from the provided hyperparameter ranges, similar as in our old random search implementation\n",
    "* initialize a model with these hyperparameters\n",
    "* train that model, and report its validation accuracy as our objective function value\n",
    "\n",
    "As you can see in the code cell below, the implementation is straight forward:\n",
    "\n",
    "PS: notice the different sampling modes, e.g. `trial.suggest_int`, `trial.suggest_loguniform` which are also similar to our previous implementation! Check out the documentation at https://optuna.readthedocs.io/en/latest/reference/trial.html to find out about further sampling modes, and **feel free to add additional hyperparameters!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # as explained above, we'll use this callback to collect the validation accuracies\n",
    "    metrics_callback = MetricsCallback()  \n",
    "    \n",
    "    # create a trainer\n",
    "    trainer = pl.Trainer(\n",
    "        #train_percent_check=1.0,\n",
    "        #val_percent_check=1.0,\n",
    "        logger=False,                                                                  # deactivate PL logging\n",
    "        max_epochs=1,                                                                  # epochs\n",
    "        gpus=0 if torch.cuda.is_available() else None,                                 # #gpus\n",
    "        callbacks=[metrics_callback],                                                  # save latest accuracy\n",
    "        early_stop_callback=PyTorchLightningPruningCallback(trial, monitor=\"val_acc\"), # early stopping\n",
    "    )\n",
    "\n",
    "    # here we sample the hyper params, similar as in our old random search\n",
    "    trial_hparams = {\"n_hidden\": trial.suggest_int(\"n_hidden\", 100, 200), \n",
    "                     \"batch_size\": 64, \n",
    "                     \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-1)}\n",
    "    \n",
    "    # create model from these hyper params and train it\n",
    "    model = MyPytorchModel(trial_hparams)\n",
    "    model.prepare_data()\n",
    "    trainer.fit(model)\n",
    "\n",
    "    # save model\n",
    "    save_model(model, '{}.p'.format(trial.number), \"checkpoints\")\n",
    "\n",
    "    # return validation accuracy from latest model, as that's what we want to minimize by our hyper param search\n",
    "    return metrics_callback.metrics[-1][\"val_acc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important difference to our previous search implementation is, that Optuna does not sample randomly! \n",
    "\n",
    "It uses a **Tree-structured Parzen Estimator (TPE)** sampler, which is a kind of bayesian optimization, and more efficient than a pure random search, because it chooses the hyperparameter values after evaluating all previous trials to make a smart guess where the best hyperparameters can be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've defined our objective function, we can start the search.\n",
    "\n",
    "For that, we can also provide a **pruner**, which is a very useful concept supported by Optuna: Similar to *early stopping*, a pruner automatically stops unpromising trials in early stages of training (a.k.a. automated early-stopping) and therefore can significantly speed up the search.\n",
    "\n",
    "If you want to specify a pruner, have a look at the available options at https://optuna.readthedocs.io/en/latest/reference/pruners.html. \n",
    "\n",
    "Then, we create our `study`, define the direction that we want to optimize (i.e., *maximize* the validation accuracy) and start the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "No environment variable for node rank defined. Set as 0.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: RuntimeWarning: You have defined a `val_dataloader()` and have defined a `validation_step()`, you may also want to define `validation_epoch_end()` for accumulating stats.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: RuntimeWarning: You have defined a `test_dataloader()` and have defined a `test_step()`, you may also want to define `test_epoch_end()` for accumulating stats.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "-----------------------------------\n",
      "0 | model   | Sequential | 3 M   \n",
      "1 | model.0 | Linear     | 3 M   \n",
      "2 | model.1 | ReLU       | 0     \n",
      "3 | model.2 | Linear     | 262 K \n",
      "4 | model.3 | ReLU       | 0     \n",
      "5 | model.4 | Linear     | 32 K  \n",
      "6 | model.5 | ReLU       | 0     \n",
      "7 | model.6 | Linear     | 1 K   \n",
      "/opt/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbb140dd8df405fa4831a89794baf55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-06-13 13:44:13,185] Finished trial#0 with value: 0.4311 with parameters: {'n_hidden': 186, 'learning_rate': 0.00010281427111728499}. Best is trial#0 with value: 0.4311.\n",
      "GPU available: False, used: False\n",
      "No environment variable for node rank defined. Set as 0.\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "-----------------------------------\n",
      "0 | model   | Sequential | 3 M   \n",
      "1 | model.0 | Linear     | 3 M   \n",
      "2 | model.1 | ReLU       | 0     \n",
      "3 | model.2 | Linear     | 262 K \n",
      "4 | model.3 | ReLU       | 0     \n",
      "5 | model.4 | Linear     | 32 K  \n",
      "6 | model.5 | ReLU       | 0     \n",
      "7 | model.6 | Linear     | 1 K   \n",
      "/opt/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: Checkpoint directory /Users/dddong/Downloads/exercise_08/checkpoints exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e771384362004dcc8c26273f481b0378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-06-13 13:45:36,852] Finished trial#1 with value: 0.4439 with parameters: {'n_hidden': 118, 'learning_rate': 0.000304667476285576}. Best is trial#1 with value: 0.4439.\n"
     ]
    }
   ],
   "source": [
    "pruner = optuna.pruners.NopPruner()\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "study.optimize(objective, n_trials=2, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we save every model in a directory called `./checkpoints`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've finished the search, we access the best trial at `study.best_trial`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 2\n",
      "Best trial:\n",
      "  Value: 0.4439\n",
      "  Params: \n",
      "    n_hidden: 118\n",
      "    learning_rate: 0.000304667476285576\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(best_trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving your best model\n",
    "\n",
    "\n",
    "When you're done with your hyperparameter search and have achieved at least 50% validation accuracy, you can save your best model to the `./models`-directory in order to submit the model.\n",
    "\n",
    "Before that, we will check again whether the number of parameters is below 5 Mio and the file size is below 20 MB.\n",
    "\n",
    "When your final model is saved, we'll lastly report the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation-Accuracy: 0.4439%\n",
      "That's too low! Please tune your model in order to reach at least 50.0% before running on the test set and submitting!\n"
     ]
    }
   ],
   "source": [
    "best_model = load_model('checkpoints/{}.p'.format(best_trial.number))\n",
    "best_model.prepare_data()\n",
    "test_and_save(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove checkpoints directory\n",
    "Lastly, let's remove the checkpoints directory again to clear up space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Here we provided just a quick overview about hyperparameter optimization using Optuna. Optuna has a lot more to offer, so definitely make sure to check out the following resources:\n",
    "\n",
    "* Source Code: https://github.com/optuna/optuna\n",
    "* Docs: https://optuna.readthedocs.io/en/stable/\n",
    "* Website: https://optuna.org\n",
    "* Paper: https://arxiv.org/pdf/1907.10902.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
